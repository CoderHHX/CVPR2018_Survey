<!DOCTYPE html><html lang="ja"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>cvpaper.challenge</title><link rel="stylesheet" href="css/reveal.css"><link rel="stylesheet" href="css/theme/white.css"><link rel="stylesheet" href="css/layout.css"><link rel="stylesheet" href="lib/css/zenburn.css"></head><script>var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script><body><div class="reveal"><div class="slides"><section><div class="paper-abstract"><div class="title">SPLATNet: Sparse Lattice Networks for Point Cloud Processing</div><div class="info"><div class="authors">Hang Su, University of Massachusetts, Amherst; Varun Jampani, NVIDIA Research; Deqing Sun, NVIDIA; Evangelos Kalogerakis, UMass; Subhransu Maji, ; Ming-Hsuan Yang, UC Merced; Jan Kautz, NVIDIA</div><div class="conference">CVPR2018</div></div><div class="slide_editor">Yue Qiu</div><div class="item1"><div class="text"><h1>�T�v</h1><p>�_�Q���𒼐ڏ����ł���SPLATNet�i�E�}�j���Ă����DSPLATNet�͒��ړ_�Q����K�w�I�ȋ�ԏ��𒊏o�\�D�܂��C2D����3D���̃}�b�s���O��s����̂ŁC�_�Q�ƃ}���`�摜�̗�����SPLATNet�ŏ����\�D�]���̒��ړ_�Q���������l�b�g���[�N�͂��Ǐ��I�ȋ�ԏ��𑹎����Ă��܂����_������D��Ď�@�͂��̖������邽�߂ɁCBCLs�w��p�����D BCLs�w�͓_�Q��X�p�[�X��lattice�Ƀ}�b�s���O���C����ɂ��̃X�p�[�X��lattice���ݍ��݂ł���D����ɂ��C unordered�_�Q������ł����ɓ_�Q�̂��Ǐ��I�ȏ����o�\�ɂ����D</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/SPLATNET.png" alt="SPLATNET"></p></div></div><div class="item3"><div class="text"><h1>�V�K���E����</h1><p>Facade segmentation�^�X�N�ɂ����āC�_�Q�ƃ}���`�摜�̃��x�����O�ɗǂ������X�s�[�h�Ə]����@��@���D�ꂽ���x�𓾂�ꂽ�DShapeNet part segmentation�ɂ����ď]����@���D�ꂽ���x�i�N���XmIoU�F83.7%�j�𓾂�ꂽ�D</p></div></div><div class="item4"><div class="text"><h1>�����N�W</h1><ul><li><a href="https://arxiv.org/abs/1802.08275">�_��</a></li></ul></div></div><div class="slide_index">[#1]</div><div class="timestamp">2018.4.13 09:55:03</div></div></section><section><div class="paper-abstract"><div class="title">From Lifestyle Vlogs to Everyday Interactions</div><div class="info"><div class="authors">Fouhey et al.</div><div class="conference">CVPR 2018.</div><div class="paper_id">arXiv ID: 1712.02310</div></div><div class="item1"><h1>概要</h1><div class="text">従来のデータ取集手法（collection-by-acting）では難しいかった, バイアスの少ない, 多様で大規模な日常生活におけるインタラクションのデータベース Lifestyle VLOG dataset を公開した. </div></div><div class="item2"><img src="slides/figs/fukuhara-From-Lifestyle-Vlogs-to-Everyday-Interactions.png"></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>従来のデータセットが想定している陽的なデータ収集とは対照的に隠的なデータ収集方法を行うことで, バイアスを小さくすることに成功した.</li><li>ビデオに対してインタラクションのラベル, フレームに対してインタラクション時の手の状態のラベル付けられている.</li><li>従来のデータセットのBiasを分析するために, 従来のデータセットで訓練した手法が Lifestyle VLOG データセットに対しても上手く動作するか検証した.</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1712.02310.pdf" target="blank">[論文] From Lifestyle Vlogs to Everyday Interactions</a></li><li><a href="https://people.eecs.berkeley.edu/~dfouhey/2017/VLOG/index.html" target="blank">Project Page</a></li><li><a href="https://github.com/dfouhey/VLOGToolkit" target="blank">GitHub</a></li></ul></div></div><div class="slide_index">[#2]</div><div class="slide_editor">Yoshihiro Fukuhara</div><div class="timestamp">2018.4.12.00:00:00</div></div></section><section><div class="paper-abstract"><div class="title">Seeing Voices and Hearing Faces: Cross-modal biometric matching</div><div class="info"><div class="authors">A. Nagrani et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Kensho Hara</div><div class="item1"><div class="text"><h1>概要</h1><p>ある音声と2人分の顔画像から，どちらの人物の声かを推定する課題と，ある顔画像と2人分の音声から，どちらの音声がその人物の声かを推定する課題の2つを解くという問題設定の研究．
異なるモダリティ間でのマッチングという課題ということ．
ある入力に対応するのがどちらの人物かという2クラス識別の問題設定として定式化．
この問題を解くために，3入力を扱う3-streamのネットワーク構造を持つモデルを提案．
音声もスペクトログラムの形式で画像のように扱い，顔画像，音声ともにConvolutionしていくモデル．
実験では80%程度の識別率を達成し，人と同等の結果が出ている．
二人分の選択肢の性別，国籍，年齢などが同じという設定にすると，60%程度の正答率になるが，こちらでは人 (57%) を上回る結果となっている．</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/Seeing_Voices_and_Hearing_Faces_Cross-modal_biometric_matching.png" alt="Seeing_Voices_and_Hearing_Faces_Cross-modal_biometric_matching.png"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><ul><li>人物の顔画像と音声の対応付けという新しい問題設定</li><li>人間レベルの高い精度を実現</li></ul></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1804.00326">論文 (arXiv)</a></li></ul></div></div><div class="slide_index">[#3]</div><div class="timestamp">2018.4.12 15:48:11</div></div></section><section><div class="paper-abstract"><div class="title">Actor and Action Video Segmentation from a Sentence</div><div class="info"><div class="authors">Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, Cees G.M. Snoek</div><div class="conference">CVPR 2018 (oral)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>センテンスの入力から、行動者と行動（Actor and Action）を同時に特定する研究である。複数の同様の物体から特定の人物など、詳細な分類が必要になる。ここではFully-Convolutional（構造の全てが畳み込みで構成される）モデルを適用してセグメンテーションベースで出力を行うモデルを提案。図は提案モデルを示す。I3Dにより動画像のエンコーディング、自然言語側はWord2Vecの特徴をさらにCNNによりエンコーディング。その後、動画像・言語特徴を統合してDeconvを繰り返しセグメントを獲得していく。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/1803ActorAction.png" alt="1803ActorAction"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>文章（と動画像）の入力から行動者と行動の位置を特定すべくセグメンテーションを実行するという問題を提起した。また、二つの有名なデータセット（A2D/J-HMDB）を拡張して7,500を超える自然言語表現を含むデータとした。同問題に対してはSoTA。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>CVxNLPの問題はここにも進出して来た。画像キャプションに限らず、この手の統合は進められるはず。</p><ul><li><a href="https://arxiv.org/pdf/1803.07485.pdf">論文</a></li></ul></div></div><div class="slide_index">[#4]</div><div class="timestamp">2018.3.24 12:47:10</div></div></section><section><div class="paper-abstract"><div class="title">Alive Caricature from 2D to 3D</div><div class="info"><div class="authors">Qianyi Wu, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>2Dの似顔絵画像から3Dの似顔絵を作成するためのアルゴリズムの提案。似顔絵画像のテストデータとしてはカリカチュアを使用し、カリカチュア画像の3Dモデルとテクスチャ化された画像を生成する。データは、標準の3D顔の変形を座標系に配置(下図、 xは口の開き具合)し、金のオリジナルデータから線形結合によって白い顔を生成する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330AC2Dto3D_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>カリカチュアを集めたデータセットを作って学習するのではなく、標準の3D顔のデータセットから実装でき、アプリケーションの柔軟さを推している。</p><p>3DMMやFaceWareHouseなどの従来手法と比較して、形の歪みが少なく、従来のものよりも綺麗な3D顔の出力が可能。顔以外にも、概形の予測が可能なオブジェクトなら応用できる？</p><ul><li><a href="https://arxiv.org/pdf/1803.06802.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180330AC2Dto3D_2.jpg"></p></div></div><div class="slide_index">[#5]</div></div></section><section><div class="paper-abstract"><div class="title">A Minimalist Approach to Type-Agnostic Detection of Quadrics in Point Clouds</div><div class="info"><div class="authors">Tolga Birdal, Benjamin Busam, Nassir Navab, Slobodan Ilic, Peter Sturm</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>オクルージョンが発生している場合/複雑な環境下でも簡単な形状がポイントクラウドから検出できる枠組みを提案する。手法は3D楕円形状のフィッティング、3次元空間操作、4点取得により構成。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180324Quadrics.png" alt="180324Quadrics"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>タイプに依存しない3次元の二次曲面（楕円球形状）検出を点群の入力から行う手法を考案した。さらに、4点探索問題を3点探索にしてRANSACベースの手法で解を求めた。モデルベースのアプローチよりはフィッティングの性能がよいが、キーポイントベースの手法よりは劣る。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>曖昧な教示のみで3次元形状探索問題が解決できるようになる？</p><ul><li><a href="https://arxiv.org/pdf/1803.07191.pdf">論文</a></li></ul></div></div><div class="slide_index">[#6]</div><div class="timestamp">2018.3.24 13:04:44</div></div></section><section><div class="paper-abstract"><div class="title">COCO-Stuff: Thing and Stuff Classes in Context</div><div class="info"><div class="authors">Holger Caesar, Jasper Uijlings, Vittorio Ferrari</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>MSCOCOデータセットに対してThing（もの）やStuff（材質）に関する追加アノテーションを行い、さらにコンテキスト情報も追加したCOCO-Stuffを提案した。このデータセットには主にシーンタイプ、そのものがどこに現れそうかという場所、物理的/材質的な属性などをアノテーションとして付与する。COCO2017をベースにして164Kに対して91カテゴリを付与し、スーパーピクセルを用いた効率的なアノテーションについてもトライした。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180329COCOStuff.png" alt="180329COCOStuff"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>材質的なアノテーションは画像キャプションに対して重要であることを確認、相対的な位置関係などデータセットのリッチなアノテーションが重要であること、セマンティックセグメンテーションベースの方法により今回のアノテーションを簡易的に行えたこと、などを示した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>さらにリッチなアノテーションは今後重要になる。この論文ではスーパーピクセルという弱い知識を用い、人間のアノテーションと組み合わせることでボトムアップ・トップダウンを効果的かつ効率的に組み合わせてアノテーションを行っている点が素晴らしい。ラストオーサのVittorio Ferrariは機械と人の協調によるアノテーションが得意（なので、既存データセットへのよりリッチなアノテーションを早いペースで提案できる）。</p><ul><li><a href="https://arxiv.org/pdf/1612.03716v4.pdf">論文</a></li><li><a href="https://github.com/nightrome/cocostuff10k">GitHub</a></li></ul></div></div><div class="slide_index">[#7]</div><div class="timestamp">2018.3.29 13:59:43</div></div></section><section><div class="paper-abstract"><div class="title">Context-aware Synthesis for Video Frame Interpolation</div><div class="info"><div class="authors">Simon Niklaus, Feng Liu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>入力フレームだけでなく、ピクセル単位の文脈情報を用いて、高品質の中間フレームを補間するためのコンテキスト認識手法の提案。まず、プレトレインモデルを使用して、入力フレームのピクセルごとのコンテキスト情報を抽出。オプティカルフローを使用して、双方向フローを推定し、入力フレームとそのコンテキストマップの両方をワープする。最後にコンテキストマップをsynthesis networkに入力し、補間フレームを生成。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401CaS.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>従来のビデオフレーム補間アルゴリズムは、オプティカルフローまたはその変動を推定し、それを用いて2つのフレーム間の中間フレームを生成する。本手法では、 2つの入力フレーム間の双方向フローを推定し、コンテキスト認識という方式をとることで精度向上を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>高品質のビデオフレーム補間実験において、従来を上回る性能。</p><ul><li><a href="https://arxiv.org/pdf/1803.10967.pdf">論文</a></li></ul></div></div><div class="slide_index">[#8]</div></div></section><section><div class="paper-abstract"><div class="title">Deep Depth Completion of a Single RGB-D Image</div><div class="info"><div class="authors">Yinda Zhang, Thomas Funkhouser</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>RGB画像から表面の法線とオクルージョン境界を予測し、 RGB-D画像と組み合わせて、欠けている奥行き情報を補完するDeep Depth Completionの提案。また、奥行き画像と対になったRGB-D画像のデータセットであるcompletion benchmark datasetを作成し、性能を評価。これは、低コストのRGB-Dカメラでキャプチャした画像と、高コストの深度センサで同時にキャプチャした画像で構成されている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401DDC.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>深度カメラは、光沢があり、明るく、透明で、遠い表面の深さを感知しないことが多い。 このような問題を解決するために、本手法ではRGB画像から得た情報と組み合わせて、 RGB-D画像の深度チャネルを完全なものにする。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>深さ修復および推定において従来よりも優れた性能。</p><ul><li><a href="https://arxiv.org/abs/1803.09326">論文</a></li><li><a href="http://deepcompletion.cs.princeton.edu/">Project webpage</a></li></ul></div></div><div class="slide_index">[#9]</div></div></section><section><div class="paper-abstract"><div class="title">Detecting and Recognizing Human-Object Interactions</div><div class="info"><div class="authors">Georgia Gkioxari, Ross Girshick, Piotr Dollár, Kaiming He</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>人物検出と同時に人物行動やその物体とのインタラクションも含めて学習を行うモデルを提案する。本論文では物体候補の中でも特にインタラクションに関係ありそうな物体に特化して認識ができるようにする。さらに、検出された<human, verb, object>のペアを用いて学習する（図の場合には<human, cut, knnife>）。さらに、その他の行動（図の場合にはstand）を同時に推定することもできる。モデルはFaster R-CNNをベースとするが、物体検出（box, class）、行動推定（action, target）、インタラクション（action）を推定して誤差を計算する。さらに、推定した人物位置に対する対象物体の方向も確率的に計算することが可能。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180322HOI.png" alt="180322HOI"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>人間に特化した検出と行動推定の枠組みを提案した。V-COCO（Verbs in COCO）にて、相対的に26%精度が向上（31.8=>40.0）、HICO-DETデータセットにて27%相対的な精度向上が見られた。計算速度は135ms/imageであり、高速に計算が可能である。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>単純な多タスク学習ではなく、人物に特化して対象物体の位置も確率的に推定しているところがGood。</p><ul><li><a href="https://arxiv.org/pdf/1704.07333.pdf">論文</a></li><li><a href="https://gkioxari.github.io/InteractNet/index.html">Project</a></li><li><a href="https://github.com/s-gupta/v-coco">Verbs in COCO DB</a></li></ul></div></div><div class="slide_index">[#10]</div><div class="timestamp">2018.3.22 19:55:34</div></div></section><section><div class="paper-abstract"><div class="title">Discriminative Learning of Latent Features for Zero-Shot Recognition</div><div class="info"><div class="authors">Minghui Yan Li, et al</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>Zero-shot learning(ZSL)における、視覚的および意味的インスタンスを別々に表現し学習するLatent Discriminative Features Learning(LDF)の提案。 (1)ズームネットワークにより差別的な領域を自動的に発見することができるネットワークの提案。(2)ユーザによって定義された属性と潜在属性の両方について、拡張空間における弁別的意味表現の学習。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330LDF.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>ZSLは、画像表現と意味表現の間の空間を学習することによって、見えない画像カテゴリを認識する。 既存の手法では、視覚と意味空間を合わせたマッピングマトリックスを学習することが中心的課題。提案手法では、差別的に学習するとうアプローチで識別精度向上を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>2つのコンポーネントによって、互いに支援しながら学習することで最先端の精度に。</p><ul><li><a href="https://arxiv.org/pdf/1803.06731.pdf">論文</a></li></ul></div></div><div class="slide_index">[#11]</div></div></section><section><div class="paper-abstract"><div class="title">Domain Adaptive Faster R-CNN for Object Detection in the Wild</div><div class="info"><div class="authors">Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, Luc Van Gool</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ドメイン変換について、ゲームなどのCG映像から実際の交通シーンに対応して物体検出を行うための学習方法を提案する。本論文では(i) 画像レベルのドメイン変換、(ii) インスタンス（ある物体）に対してのドメイン変換、の二種類の方法を提案し、整合性をとるように正規化する（図のConsistency Regularization; Global/Localな特徴変換を考慮）。ここで、物体検出はFaster R-CNNをベースとしてドメイン変換の手法も二種類（H-divergence、敵対的学習）用意する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314DomainFRCNN.png" alt="180314DomainFRCNN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>CGで学習し実環境における自動運転などで使えるドメイン変換の手法を提案した。実験はCityscapes, KITTI, SIM10Kなどで行い、ロバストな物体検出を実行することができた。例えばCityscapesとKITTIの相互ドメイン変換でベースラインのFaster R-CNNが30.2 (K->C)、53.5 (C->K)のところ、Domain Adaptive Faster R-CNNでは38.5 (K->C)、64.1 (C->K)であった。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>データ収集は手動から自動の時代になって来た？データを手作業で集める時代からアルゴリズムを駆使して収集する時代へ移行。</p><ul><li><a href="https://arxiv.org/pdf/1803.03243.pdf">論文</a></li><li><a href="http://www.vision.ee.ethz.ch/~liwenw/">著者</a></li></ul></div></div><div class="slide_index">[#12]</div><div class="timestamp">2018.3.14 08:43:53</div></div></section><section><div class="paper-abstract"><div class="title">Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++</div><div class="info"><div class="authors">David Acuna, Huan Ling, Amlan Kar, Sanja Fidler</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>Polygon-RNNのアイデアを踏襲し、ヒューマン・イン・ザ・ループを使って対話的にオブジェクトのポリゴンアノテーションの生成。また、新しいCNNエンコーダアーキテクチャの設計、強化学習によるモデルの効果的な学習、 Graph Neural Networkを使用した出力解像度の向上を行う。これらのアーキテクチャをPolygon-RNN ++と呼ぶ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180331PolygonRNN++_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>アノテーション作成時の負担を軽減。より正確にアノテーションを付加できるため、雑音の多いアノテーターに対しても頑健である。</p><p>高い汎化能力となり、既存のピクセルワイズメソッドよりも大幅に改善。ドメイン外のデータセットにも適応可能。</p><ul><li><a href="https://arxiv.org/pdf/1803.09693.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180331PolygonRNN++_2.jpg"></p></div></div><div class="slide_index">[#13]</div></div></section><section><div class="paper-abstract"><div class="title">Egocentric Basketball Motion Planning from a Single First-Person Image</div><div class="info"><div class="authors">Gedas Bertasius, Aaron Chan, Jianbo Shi</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>一人称視点の画像からゴールリングに到達するまでのバスケットボール選手の動線を生成する。本論文では3D位置や頭部方向も記録する。同タスクを実行するため、まずは画像空間から12Dのカメラ空間に投影を行うEgoCam CNNを学習。次に予測を行うCNN（Future CNN）を構築、さらに予測位置やゴールまでの位置が正確かどうかを検証するGoal Verifier CNNを用いることでより正確な推定を行うことができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180307EgoBasketball.png" alt="180307EgoBasketball"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>複数のネットワークの出力（ここではEgoCamCNNとFutureCNN）を検証するVerification Networkという考え方は面白い。他のネットワークの出力を、検証用のネットワークにより正すというのはあらゆる場面で用いることができる。RNN/LSTM/GANsなどよりも高度な推定ができることが判明した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>結果例は動画像を参照。未来予測・３次元投影などコンポーネントがDNNにより高度にできるようになってきたからできた研究。さらに検証用のネットワークを構築することで出力自体を操作している。</p><ul><li><a href="https://arxiv.org/pdf/1803.01413v1.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=wRRRl4QsUQg">YouTube</a></li></ul></div></div><div class="slide_index">[#14]</div><div class="timestamp">2018.3.7 09:04:15</div></div></section><section><div class="paper-abstract"><div class="title">Fast and Accurate Single Image Super-Resolution via Information Distillation Network</div><div class="info"><div class="authors">Zheng Hui, Xiumei Wang, Xinbo Gao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>元の低解像度画像から高解像度画像を再構築するための、深くてコンパクトなCNNを提案。提案モデルは、特徴抽出ブロック、積み重ね情報蒸留ブロック、再構成ブロックの3部構成。これにより、情報量が豊富かつ効率的に特徴を徐々に抽出できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180331FaASISR.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>CNNが超解像殿画像を扱うようになってきたが、ネットワークが増大するにつれて、計算上の複雑さとメモリ消費という問題が生じる。これらの問題を解決するためのコンパクトなCNN。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>PSNR、SSIM、IFCの4つのデータセットで検証し、精度向上を確認。デシジョンおよび圧縮アーチファクト低減などの他の画像修復問題にも応用可能？</p><ul><li><a href="https://arxiv.org/pdf/1803.08679.pdf">論文</a></li></ul></div></div><div class="slide_index">[#15]</div></div></section><section><div class="paper-abstract"><div class="title">Future Frame Prediction for Anomaly Detection -- A New Baseline</div><div class="info"><div class="authors">Wen Liu, Weixin Luo, Dongze Lian, Shenghua Gao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>先の（未来の）フレーム予測と異常検知を同時に行う手法を提案する論文。予測したフレームと異常検知の正解値により誤差を計算して最適化を行う。図に本論文で提案するネットワークアーキテクチャの図を示す。U-Netにより画像予測やさらにオプティカルフロー推定を行い、RGB空間、オプティカルフロー空間にて誤差を計算しGANの枠組みでそれらがリアルかフェイクかを判定する。同フレームを用いて異常検知を実施する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180315PredictionAnomaly.png" alt="180315PredictionAnomaly"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>従来は現在フレームを入力として異常検知を行う手法は存在したが、未来フレームを予測して異常検知を行う枠組みは本論文による初めての試みである。異常値の正解値を与えることで画像予測にもフィードバックされるため、画像予測と異常検知の相互学習に良い影響を与える。オープンデータベースにてベンチマークした結果、何れもState-of-the-artな精度を達成。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>生成ベースで画像予測+X（Xは任意タスク）というものはSoTAが出せるくらいにはなってきた。</p><ul><li><a href="https://arxiv.org/pdf/1712.09867.pdf">論文</a></li><li><a href="https://github.com/StevenLiuWen/ano_pred_cvpr2018">Project</a></li></ul></div></div><div class="slide_index">[#16]</div><div class="timestamp">2018.3.15 09:04:03</div></div></section><section><div class="paper-abstract"><div class="title">Guided Labeling using Convolutional Neural Networks</div><div class="info"><div class="authors">Sebastian Stabinger, et al. </div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルの付いていないデータに対して、どの画像にラベルを付けてデータセットを構成すればよいかを判断するguided labelingの提案。ラベル付けを行う必要があるサンプルを見定めることで、データセットの量を大幅に減らすことができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180313GuidedLabeling.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>大規模データセットにおいて、手動でのラベル付けは大変。選別してラベル付けを行えば、作業を最小限に抑えられる。また、ある意味良いデータを選別できるため、場合によっては精度も向上。</p></div></div><div class="item4"><div class="text"><p>MNISTは、データセットのサイズを1/16に、CIFAR10は1/2に減らすことが可能に。また、MNISTの場合は、全部使った時よりも識別精度が向上した。普遍性を妨げる不必要なデータを取り除けたことが精度向上につながった？</p><ul><li><a href="https://arxiv.org/pdf/1712.02154.pdf">論文</a></li></ul></div></div><div class="slide_index">[#17]</div></div></section><section><div class="paper-abstract"><div class="title">HATS: Histograms of Averaged Time Surfaces for Robust Event-based Object Classification</div><div class="info"><div class="authors">Amos Sironi, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>イベントベースカメラにおける、識別アルゴリズムの提案。本研究では、(1)イベントベースのオブジェクト分類のための低レベル表現とアーキテクチャの欠如、(2)実世界における大きなイベントベースのデータセットの欠如、の2つの問題に取り組む。新しい機械学習アーキテクチャ、イベントベースの特徴表現(Histograms of Averaged Time Surfaces)、データセット(N-CARS)を提案。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330NCARS.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>イベントベースのカメラは、従来のフレームベースのカメラと比較して、高時間分解能、低消費電力、高ダイナミックレンジという点で優れており、様々なシーンで応用が利く。しかし、イベントベースのオブジェクト分類アルゴリズムの精度は未だ低い。特徴表現には過去時間の情報を使用。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>過去の情報を使うことで、既存のイベントベースカメラによる認識手法よりも優れた結果となった。</p><ul><li><a href="https://arxiv.org/pdf/1803.07913.pdf">論文</a></li><li><a href="http://www.prophesee.ai/dataset-n-cars/">データセット</a></li></ul></div></div><div class="slide_index">[#18]</div></div></section><section><div class="paper-abstract"><div class="title">Improving Object Localization with Fitness NMS and Bounded IoU Loss</div><div class="info"><div class="authors">Lachlan Tychsen-Smith, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>既存のNon-Max Supressionを改良したFitness NMSの提案。Soft NMSも同時に使用するとより効果的。</p><p>勾配降下法の収束特性(滑らかさ、堅牢性など)を維持しつつ、IoUを最大化するという目標により適した損失関数であるBounded IoU Loss の提案。これをRoIクラスタリングと組み合わせることで精度が向上する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314FitnessNMS.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>バウンディングボックスのスコアを算出する関数を拡張する。具体的には、グランドトゥルースとのIoUと、クラスの期待値を追加する。これにより、IoUの重なり推定値と、クラス確率の両方が高いバウンディングボックスを優先して学習することができる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>MSCOCO、Titan X(Maxwell)使用時では、精度33.6％-79Hzまたは41.8％-5Hz。本論文ではDeNetでテストしたが、別の手法でも精度向上が望めるよう。</p><ul><li><a href="https://arxiv.org/pdf/1711.00164.pdf">論文</a></li><li><a href="https://github.com/lachlants/denet">ソースコード</a></li></ul></div></div><div class="slide_index">[#19]</div></div></section><section><div class="paper-abstract"><div class="title">Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN</div><div class="info"><div class="authors">Shuai Li, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>新しいRNN手法であるindependently recurrent neural network (IndRNN)の提案。一枚のレイヤ内のニューロンが独立しており、レイヤ間で接続されている。これにより、勾配消失問題や爆発問題を防ぎ、より長期的なデータを学習することができる。また、IndRNNは複数積み重ねることができるため、既存のRNNよりも深いネットワークを構築できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314IndRNN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>本手法によって下記の従来手法の問題を解決。</p><p>RNNは、勾配の消失や爆発の問題、長期パターンの学習が困難である。LSTMやGRUは、上記のRNNの問題を解決すべく開発されたが、層の勾配が減衰してしまう問題がある。また、RNNは全てのニューロンが接続されているため、挙動の解釈が困難。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>かなり長いシーケンス(5000回以上の時間ステップ)を処理でき、かなり深いネットワーク（実験では21レイヤー）を構築できる。</p><ul><li><a href="https://arxiv.org/pdf/1803.04831.pdf">論文</a></li></ul></div></div><div class="slide_index">[#20]</div></div></section><section><div class="paper-abstract"><div class="title">Iterative Visual Reasoning Beyond Convolutions</div><div class="info"><div class="authors">Xinlei Chen, Li-Jia Li, Li Fei-Fei, Abhinav Gupta</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>CNNのような理由を突き止める能力がない認識システムを超えた、反復的なvisual reasoningのための新しいフレームワークの提案。畳み込みベースのローカルモジュールとグラフベースのグローバルモジュールの2コアで構成。2つのモジュールのを繰返し展開し、予測結果を相互にクロスフィードして絞り込む。最後に、両方のモジュールの最高値をアテンションベースのモジュールと組み合わせてプレディクト。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401IVRBC_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>ただ畳み込むだけでなく、Spatial(空間的)およびSemanticの空間を探索することができる。下図のように、「人」は「車」を運転するというSpatialとSemanticの双方を兼ね備えた認識を行うことで精度向上を図る。</p><p>通常のCNNと比較して、ADEで8.4％、COCOで3.7％の精度向上。</p><ul><li><a href="https://arxiv.org/pdf/1803.11189.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180401IVRBC_2.jpg"></p></div></div><div class="slide_index">[#21]</div></div></section><section><div class="paper-abstract"><div class="title">LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image</div><div class="info"><div class="authors">Chuhang Zou, Alex Colburn, Qi Shan, Derek Hoiem</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>単一のパースペクティブまたはパノラマ画像から屋内3Dルームレイアウトを推定するLayoutNetの提案。最初に、消失点を分析し、水平になるように画像を整列。これにより、壁と壁の境界が垂直になり、ノイズ低減。画像からコーナー(レイアウト接合点)と境界を、エンコーダ/デコーダ構造のCNNで出力。最後に、3D Layoutパラメータを、予測したコーナーと境界に適合するように最適化する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401LayoutNet.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>アーキテクチャはRoomNetと似ているが、消失点に基づいて画像を整列させ、複数のレイアウト要素（コーナー、境界線、サイズ、平行移動）を予測し、 “L”形の部屋のような非直方体のマンハッタンレイアウトに対しても適応できる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>従来手法と比較して、処理速度と正確さにおいて性能の向上。</p><ul><li><a href="https://arxiv.org/pdf/1803.08999.pdf">論文</a></li><li><a href="https://github.com/zouchuhang/LayoutNet">ソースコード</a></li></ul></div></div><div class="slide_index">[#22]</div></div></section><section><div class="paper-abstract"><div class="title">Learning to Localize Sound Source in Visual Scenes</div><div class="info"><div class="authors">Arda Senocak, Tae-Hyun Oh, Junsik Kim, Ming-Hsuan Yang, In So Kweon</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>画像と音声の入力から、音が画像のどこで鳴っているか（鳴りそうか？）を推定した研究。さらに、人の声なら人の領域、車の音なら車の領域にアテンションがあたるなど物体と音声の対応関係も学習することができる。学習には音源とその対応する物体の位置を対応づけたデータセット（144Kのペアが含まれるSound Source Localization Dataset）を準備した。さらに既存の物体認識と音声を対応づけて（？）Unsupervised/Semi-supervisedに学習することにも成功した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180322LocalizeSound.png" alt="180322LocalizeSound"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>教師あり、教師なし、半教師あり、いずれの枠組みでも音声ー物体の対応関係を学習することができるようにした。音源とそれに対応する物体領域の尤度がヒートマップにて高く表示されている。結果はビデオを参照されたい。教師なし学習はTriplet-lossにより構成され、ビデオと近い/遠い音声の誤差により計算。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>非常に面白い問題設定、プラス誤差関数を柔軟に抽出可能というところが上手。精読しても良いと感じた論文。CVにおいてビデオの音声は今まで使用しないことも多かった（もしくは精度向上のために活用していた）が、これからは使用方法を見直してもよいと感じた。</p><ul><li><a href="https://arxiv.org/pdf/1803.03849.pdf">論文</a></li><li><a href="https://www.youtube.com/watch?v=UyairkbzR_Y">YouTube</a></li></ul></div></div><div class="slide_index">[#23]</div><div class="timestamp">2018.3.22 19:18:32</div></div></section><section><div class="paper-abstract"><div class="title">Learning to Segment Every Thing</div><div class="info"><div class="authors">Ronghang Hu, Piotr Dollár, Kaiming He, Trevor Darrell, Ross Girshick</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルが完全に手に入らない際にでも転移学習が可能なセグメンテーション手法（論文中ではPartially Supervised Training Paradigm, weight transfer functionを紹介）を提案する。条件として、bboxが手に入っている物体に対してセグメンテーション領域を学習可能。Mask R-CNNをベースとしているが、Weight Transfer Functionを追加、セグメントの重みを学習・推定して誤差計算と学習繰り返し。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180303SegmentEverything.png" alt="180303SegmentEverything"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Visual Genome Datasetから3,000の視覚的概念を獲得、MSCOCOから80のマスクアノテーションを獲得した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>弱教師付き学習が現実的な精度で動作するようになってきた？アノテーションはお金や知識があっても非常に大変なタスクであり、いかに減らすかという方向に研究が進められている。（What's next?ー弱教師/教師なしの先とは？）</p><ul><li><a href="https://arxiv.org/pdf/1711.10370.pdf">論文</a></li><li><a href="http://ronghanghu.com/">著者</a></li><li><a href="http://kaiminghe.com/">Kaiming He</a></li></ul></div></div><div class="slide_index">[#24]</div><div class="timestamp">2018.3.3 10:46:40</div></div></section><section><div class="paper-abstract"><div class="title">MakeupGAN: Makeup Transfer via Cycle-Consistent Adversarial Networks</div><div class="info"><div class="authors">Huiwen Chang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ソース画像のメイクをターゲット画像へ転写やメイクの除去をする研究。ターゲット画像とメイク済み画像の2枚を入力としメイクを転写するネットワークGとメイク済み画像らメイクを取り除くネットワークFを考え、2つのネットワークによって元の画像に戻るように学習していく。
その際、Fによってxに付与されたメイクがyのメイクと同じものであるかを評価するロスを加えることでメイクの特徴を捉える。
従来手法ではメイク転写・除去を独立した問題として考えていたが、この研究ではセットとして考えている。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180408make.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Youtubeのメイクチュートリアルの動画から、1148枚のメイクなし画像と1044枚のメイクあり画像を収集。ユーザースタディによって2つの既存手法と比較し、提案手法が一番いいと答えた人が65.7％（2番目と答えた人が31.4％）
従来手法では肌の色や表情の違いがあると上手くいかないのに対し、ソースとターゲット間でこれらが違ってもうまく転写できる。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://research.adobe.com/project/makeupgan-makeup-transfer-via-cycle-consistent-adversarial-networks/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#25]</div><div class="timestamp">2018.4.8 01:45:44</div></div></section><section><div class="paper-abstract"><div class="title">Motion-Appearance Co-Memory Networks for Video Question Answering</div><div class="info"><div class="authors">Jiyang Gao, Runzhou Ge, Kan Chen, Ram Nevatia</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>ビデオQAのための、 Dynamic Memory Network(DMN) のコンセプトに基づいたmotion-appearance comemory networkの提案。本研究の特徴は次の3つである。(1)アテンションを生成するために動きと外観情報の両方を手がかりとして利用する共メモリアテンションメカニズム。(2) multi-level contextual factを生成するための時間的conv-deconv network。(3)異なる質問に対して動的な時間表現を構成するdynamic fact ensemble method。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401MACoMN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>本手法は、次のようなvideo QA特有の属性に基づいている。(1)豊富な情報を含む長い画像シーケンスを扱う。(2)動き情報と出現情報を相互に関連付け、アテンションキューを他の情報に応用できる。(3)答えを推論するために必要なフレーム数は質問によって異なる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>TGIF-QAの4つのタスクすべてにおいて、最先端技術よりも優れている。</p><ul><li><a href="https://arxiv.org/pdf/1803.10906.pdf">論文</a></li></ul></div></div><div class="slide_index">[#26]</div></div></section><section><div class="paper-abstract"><div class="title">Multi-Frame Quality Enhancement for Compressed Video</div><div class="info"><div class="authors">Ren Yang, Mai Xu, Zulin Wang, Tianyi Li</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>圧縮した動画像に対して画質を向上させる研究。Peak Quality Frames (PQFs)を用いたSVMベースの手法やMulti-Frame CNN (MF-CNN)を提案。提案法により、圧縮動画における連続フレームからアーティファクトを補正するような働きが見られた。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180324PQF.png" alt="180324PQF"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>動画の画質改善手法においてState-of-the-art。動画に対する画質改善の結果は図を参照。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.04680">論文</a></li><li><a href="https://github.com/ryangBUAA/MFQE">GitHub</a></li></ul></div></div><div class="slide_index">[#27]</div><div class="timestamp">2018.3.24 15:14:35</div></div></section><section><div class="paper-abstract"><div class="title">Multi-Level Factorisation Net for Person Re-Identification</div><div class="info"><div class="authors">Xiaobin Chang, Timothy M. Hospedales, Tao Xiang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>人間の視覚的外観を、人の手によるアノテーションなしかつ、複数のセマンティックレベルで識別因子に分解する Multi-Level Factorisation Net(MLFN)の提案。 MLFNは、複数のブロックで構成されており、各ブロックには、複数の因子モジュールと、各入力画像の内容を解釈するための因子選択モジュールが含まれている。 </p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180331MLFN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>効果的なRe-IDを目指すには、高低のセマンティックレベルでの人の差別化かつ視界不変性をモデル化することである。 近年(2018)のdeep Re-IDモデルは、セマンティックレベルの特徴表現を学習するか、アノテーション付きデータが必要となる。MLFNではこれらを改善する。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>3つのRe-IDと、CIFAR-100の結果で最先端。</p><ul><li><a href="https://arxiv.org/pdf/1803.09132.pdf">論文</a></li></ul></div></div><div class="slide_index">[#28]</div></div></section><section><div class="paper-abstract"><div class="title">Non-local Neural Networks </div><div class="info"><div class="authors">Xiaolong Wang et al.</div><div class="conference">CVPR 2018</div></div><div class="item1"><h1>概要</h1><div class="text">NLPなどで効果を発揮しているself-attentionを多次元に一般化し、2D/3DCNNに導入することで新たな「non-local block」を形成し、画像や動画での実験を行った。
行動認識＠Kineticsでは非常に高い精度を達成。Instance segmentationやkey point detectionなどのタスクでも汎用的に効果を発揮。
</div></div><div class="item2"><img src="slides/figs/non_local.png"></div><div class="item3"><h1>手法</h1><div class="text">位置jと位置iに依存してアテンションを出力する関数f(.)とjのみに依存する関数g(.)の積を入力位置jに関して和をとることによって位置iの出力値を決定する。
位置情報の保存、可変入力サイズ、などの性質を持ち、全結合、畳み込みを特殊な形として含む。またf(.)の定義の仕方によってはself-attentionと一致する。
f(.)は様々な形が提案されているが、種類によらず効果を発揮している。実際に使用する場合は図のような残差構造を使用している。

</div></div><div class="item4"><h1>自由記述欄</h1><div class="text"><h1>コメント・リンク</h1><p></p>効果のインパクトがすごい。学習曲線からもうまくいっていることが明らか。C2Dに対してspace-timeにnon-local blockを適用すると3Dconvよりも時系列方向への拡大として効果があったのが興味深い。
結局残差を用いたnon-local blockを使用していたので、単純にnon-local layerのみでの性能もきになる。
位置情報の保存は重要でも、局所性はあまり重要ではなかったのかと感じられる。<ul><li><a href="https://arxiv.org/abs/1711.07971">論文</a></li></ul></div></div><div class="slide_index">[#29]</div><div class="slide_editor">Tomoyuki Suzuki</div></div></section><section><div class="paper-abstract"><div class="title">Pose-Robust Face Recognition via Deep Residual Equivariant Mapping</div><div class="info"><div class="authors">Kaidi Cao, Yu Rong, Cheng Li, Xiaoou Tang, Chen Change Loy</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>横顔の認識精度を高めるためにDeep Residual EquivAriant Mapping (DREAM)の提案。正面と側面の顔間のマッピングを行うことで特徴空間を対応付ける。これにより、横顔を正面の姿勢に変換して認識を単純化。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180313DREAM_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・手法・リンク集</h1><p>正面と側面のトレーニング数の不均衡から、現代の顔認識モデルの多くは、正面と比べて横顔を処理するのが比較的貧弱。本手法は姿勢変動を伴う顔認識に限定されない顔認識が可能で、横顔のデータを増やさなくても精度向上。</p><p>上図より、DREAMをCNNに追加し、入力に残差を動的に追加。下図はマッピングによる姿勢変換の例。</p><ul><li><a href="https://arxiv.org/pdf/1803.00839.pdf">論文</a></li><li><a href="http://mmlab.ie.cuhk.edu.hk/projects/DREAM">ソースコード</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180313DREAM_2.jpg"></p></div></div><div class="slide_index">[#30]</div></div></section><section><div class="paper-abstract"><div class="title">Pyramid Stereo Matching Network</div><div class="info"><div class="authors">Jia-Ren Chang, Yong-Sheng Chen</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>空間ピラミッドプーリングと3D CNNの2つのモジュールから構成された、ステレオ画像対からの奥行き推定を行うPyramid Stereo Matching Network(PSMNet)の提案。空間ピラミッドプーリングは、異なるスケールおよび位置でコンテキストを集約し、コストボリュームを形成する。 3D CNNは、複数のhourglass networksを重ねて、コストボリュームを規則化することを学習。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330PSMN.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>現在(2018)ではステレオ画像からの奥行き推定を、CNNの教師あり学習で解決されてきている。 コンテキスト情報を利用することで精度向上を図る。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>最先端の手法よりも優れている結果。</p><ul><li><a href="https://arxiv.org/pdf/1803.08669.pdf">論文</a></li><li><a href="https: //github.com/JiaRenChang/PSMNet">ソースコード</a></li></ul></div></div><div class="slide_index">[#31]</div></div></section><section><div class="paper-abstract"><div class="title">Referring Relationships</div><div class="info"><div class="authors">Ranjay Krishna, Ines Chami, Michael Bernstein, Li Fei-Fei</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>referring relationshipsを利用して同カテゴリのエンティティ間の曖昧さを解消するタスクの提案。特徴抽出後、アテンションを生成。述語を使用することで、アテンションをシフトさせる。この述語シフトモジュールを介して、subjectとobjectの間でメッセージを反復的に渡すことで、2つのエンティティをローカライズ。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401ReferringRelationships_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>画像中のエンティティ間の関係にはそれぞれ意味があり、画像の理解に役立つ。例えば、図のサッカーの試合の画像では、複数の人写っているが、それぞれは異なる関係を持っている。一人はボールを蹴っており、もう一人はゴールを守っている。 <person-kicking-ball>に着目すると、述語の”kick”を理解することにより、画像内のどの人物が”ball”を蹴っているのかを正しく識別する。</p><ul><li><a href="https://arxiv.org/pdf/1803.10362.pdf">論文</a></li><li><a href="https://github.com/StanfordVL/ReferringRelationships">ソースコード</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180401ReferringRelationships_2.jpg"></p></div></div><div class="slide_index">[#32]</div></div></section><section><div class="paper-abstract"><div class="title">Rethinking Feature Distribution for Loss Functions in Image Classification</div><div class="info"><div class="authors">Weitao Wan, Yuanyi Zhong, Tianpeng Li, Jiansheng Chen</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>本論文ではLarge-margin Gaussian Mixture (L-GM) Lossを提案して画像識別タスクに応用する。Softmax Lossとの違いは、学習セットにおけるディープ特徴の混合ガウス分布をフォローしつつ仮説を設定するところである。識別境界や尤度正則化においてL-GM Lossは非常に高いパフォーマンスを実現している。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180314LGM.png" alt="180314LGM"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>L-GM Lossは画像識別においてSoftmax Lossよりも精度が高いことはもちろん、特徴分布を考慮するため例えばAdversarial Examples（摂動ノイズ）などにおいても対応できる。MNIST, CIFAR, ImageNet, LFWにおける識別や摂動ノイズを加えた実験においても良好な性能を確かめた。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>Softmax Lossよりも有意に精度向上が見られている。導入が簡単なら取り入れて精度向上したい。</p><ul><li><a href="https://arxiv.org/pdf/1803.02988.pdf">論文</a></li></ul></div></div><div class="slide_index">[#33]</div><div class="timestamp">2018.3.14 11:04:45</div></div></section><section><div class="paper-abstract"><div class="title">Robust Depth Estimation from Auto Bracketed Images</div><div class="info"><div class="authors">Sunghoon Im, Hae-Gon Jeon, In So Kweon</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>HDRの画像の明るさを補正するためのブラケット撮影からの距離画像やカメラ姿勢を同時推定する手法を提案する論文。ブラケット撮影とは通常の露出撮影以外に意図的に「少し明るめの写真」と「少し暗めの写真」を同時に撮影。距離画像推定は幾何変換をResidual-flow Networkに統合したモデルにより行う。ここでは学習ベースのMulti-view stereo手法（Deep Multi-View Stereo; DMVS）を幾何推定（Structure-from-Small-Motion; SfSM）と組み合わせる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180323BracketedImages.png" alt="180323BracketedImages"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>距離画像推定において、スマートフォンやDSLRカメラなど種々のデータセットにてSoTAな精度を達成。モバイル環境でも動作するような小さなネットワークと処理速度についても同時に実現した。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/abs/1803.07702">論文</a></li><li><a href="https://sunghoonim.github.io/">著者</a></li></ul></div></div><div class="slide_index">[#34]</div><div class="timestamp">2018.3.23 19:11:04</div></div></section><section><div class="paper-abstract"><div class="title">Rotation-Sensitive Regression for Oriented Scene Text Detection</div><div class="info"><div class="authors">Minghui Liao, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>自然画像から文字を検出する。単なる検出ではなく、文字の方向を考慮したバウンディングボックスによる検出手法であるRotation-sensitive Regression Detector (RRD)の提案。回帰ブランチによって、畳み込みフィルタを回転させて回転感知特徴を抽出。分類ブランチによって、回転感性特徴をプーリングすることによって回転不変特徴を抽出。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180329RRD.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>文字をテーマにした研究では(1)テキストの向きを無視した分類方法と，(2)向きを考慮したバウンディングボックスによる回帰がある。従来研究では、両方のタスクの共有の特徴を使用していたが、互換性がなかったためにパフォーマンスが低下(図b)。そこで、異なる2つのネットワークから抽出した、異なる特性の特徴を分類および回帰することを提案(図d,e)。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>ICDAR 2015、MSRA-TD500、RCTW-17およびCOCO-Textを含む3つのシーンテキストのデータセットで最先端のパフォーマンスを達成。向きがある一般物体検出にも応用可能？</p><ul><li><a href="https://arxiv.org/pdf/1803.05265.pdf">論文</a></li></ul></div></div><div class="slide_index">[#35]</div></div></section><section><div class="paper-abstract"><div class="title">SketchMate: Deep Hashing for Million-Scale Human Sketch Retrieval</div><div class="info"><div class="authors">Peng Xu, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>スケッチ検索のためのディープハッシングフレームワークの提案。3.8mの大規模スケッチデータセットを構築。CNNでスケッチの特徴抽出。RNNでペンストロークの時間情報をモデル化。CNN-RNNでエンコードすることで、スケッチ性質に対応した新しいhashing lossを導入。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180408SkechMate.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・差分</h1><p>従来のスケッチ認識タスクに従う代わりに、より困難な問題のスケッチハッシュ検索を行う。ネットワークをスケッチ認識のために再利用することもでき、どちらも高パフォーマンス。大規模なデータセットを利用することで、従来の文献ではあまり研究されていなかった、スケッチのユニークな特性を見出す。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1804.01401.pdf">論文</a></li><li><a href="http://sketchx.eecs.qmul.ac.uk/downloads/">ソースコード/データセット</a></li></ul></div></div><div class="slide_index">[#36]</div></div></section><section><div class="paper-abstract"><div class="title">Style Aggregated Network for Facial Landmark Detection</div><div class="info"><div class="authors">Xuanyi Dong, Yan Yan, Wanli Ouyang, Yi Yang, University of Technology Sydney, The University of Sydney</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>顔のランドマーク検出。顔そのもののばらつきの他に、グレースケールやカラー画像、明暗などの画像スタイルが変わっても同様に検出できるStyle Aggregated Network(SAN)の提案。まず、(1)入力画像をさまざまなスタイルに変換し、スタイルを集約し、(2)顔のランドマーク予測する。(2)は、元画像とスタイルを集約した特徴の両方を入力し、融合してカスケード式のヒートマップ予測を生成する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330SAN_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>結果・リンク集</h1><p>Flickr8kとFlickr30kを使った実験において、最先端モデルと同等かそれ以上の結果。より正確で、より多様なキャプション生成。</p><ul><li><a href="https://arxiv.org/pdf/1803.04108.pdf">論文</a></li><li><a href="https://github.com/D-X-Y/SAN">ソースコード</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180330SAN_2.jpg"></p></div></div><div class="slide_index">[#37]</div></div></section><section><div class="paper-abstract"><div class="title">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</div><div class="info"><div class="authors">Richard Zhang et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="https://sites.google.com/site/shinatoyamamoto/" target="blank">ShintaroYamamoto</a></div><div class="item1"><div class="text"><h1>概要</h1><p>2枚の画像の類似度を表す指標は数多く提案されているが、その類似度は必ずしも人間の知覚と一致していない。近年はDNNにより高次の特徴を得ることが可能となっており、人間の知覚に近づいている。
そこで、既存の類似度の評価尺度とDNNベースの類似度判定を比較することでDNNベースの手法がより人間の知覚に近い類似度を表現できることを確認した。
具体的には、ある画像を異なる方法で加工したもの2つを用意し、どちらが元の画像に近いかを人間とコンピュータ両方に判定させることで検証を行った。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180408perception.png" alt="Item3Image"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>データセットとして、画像に様々な加工を施したデータを人間に類似度を評価してもらったものを作成。加工の例としては、ノイズの付与やオートエンコーダによる画像の復元などが挙げられる。
検証の結果、ＤＮＮベースの類似度の方が既存の尺度より人間の知覚に乗っ取ってることを示した。
また、DNNのネットワーク構造そのものは重要ではないことが分かった。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://richzhang.github.io/PerceptualSimilarity/">プロジェクトページ</a></li></ul></div></div><div class="slide_index">[#38]</div><div class="timestamp">2018.4.8 01:36:55</div></div></section><section><div class="paper-abstract"><div class="title">TOM-Net: Learning Transparent Object Matting from a Single Image</div><div class="info"><div class="authors">Guanying Chen, Kai Han, Kwan-Yee K. Wong</div><div class="conference">CVPR 2018 (spotlight)</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>透明物体の切り抜き（Transparent Object Matting; TOM）と反射特性を推定することが可能なネットワークTOM-Netを提案する。TOM-Netにより、物体の反射特性を保存しながら他の画像にレンダリングして、同画像のテクスチャを反映させることができる。同問題を反射フローの推定問題と捉えてDNNのモデルを構築することで解決した。荒い部分は多階層のEncoder-Decorderで推定し、詳細な部分はResidualNetで調整する。この問題を解決するために、データセットを構築した。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180324TOMNet.png" alt="180324TOMNet"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>178Kの画像を含むデータセットを構築した。同DBには876サンプル、14の透明物体、60種の背景を含む。透明物体の推定と反射特性のレンダリングはGitHubページを参照。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.04636.pdf">論文</a></li><li><a href="http://gychen.org/TOM-Net/">Project</a></li><li><a href="https://github.com/guanyingc/TOM-Net_Rendering">GitHub</a></li></ul></div></div><div class="slide_index">[#39]</div><div class="timestamp">2018.3.24 18:05:46</div></div></section><section><div class="paper-abstract"><div class="title">Towards Human-Machine Cooperation:Self-supervised Sample Mining for Object Detection</div><div class="info"><div class="authors">Keze Wang, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>物体検出の課題を考慮し、既存のActive Learning(AL)の欠点を改善することを目的とした、Self-Supervised Sample Mining(SSM)の提案。ラベルなし、もしくは一部ラベルのないデータを使って学習することができる。交差検証後のスコアによってサンプルを選別。低い場合にはユーザによってアノテーション、高い場合にはそのままラベルとして採用。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330SSM.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>既存のAL法では主に、単一の画像コンテクスト内でサンプル選択基準を定義し、大規模な物体検出において最適ではなく、頑強性および非実用的である。SSMによって、ユーザが必要な部分にだけ介入し、アノテーションの作業を軽減。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>アノテーションが少ないデータセットにおいても最先端の精度。</p><ul><li><a href="https://arxiv.org/pdf/1803.09867.pdf">論文</a></li></ul></div></div><div class="slide_index">[#40]</div></div></section><section><div class="paper-abstract"><div class="title">Towards Open-Set Identity Preserving Face Synthesis</div><div class="info"><div class="authors">Jianmin Bao, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>顔画像からidentityとattributesを別々に再構成する、GANに基づいたOpen-Set Identity Generating Adversarial Networkの提案。 face synthesis networkは、ポーズや感情、照明、背景などをキャプチャする属性ベクトルを抽出することができる。図中の2つの入力画像AおよびBから抽出された識別を再結合することによって、A0およびB0を生成することができる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401OSIPFS_1.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果・リンク集</h1><p>顔の正面化、顔属性モーフィング、 face adversarial example detectionなど、より広範なアプリケーションに応用可能。</p><ul><li><a href="https://arxiv.org/pdf/1803.11182.pdf">論文</a></li></ul></div></div><div class="item4"><div class="text"><p><img src="slides/figs/180401OSIPFS_2.jpg"></p></div></div><div class="slide_index">[#41]</div></div></section><section><div class="paper-abstract"><div class="title">Towards Universal Representation for Unseen Action Recognition</div><div class="info"><div class="authors">Yi Zhu, Yang Long, Yu Guan, Shawn Newsam, Ling Shao</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>学習画像がなくても行動認識を実現する「Unseen Action Recognition (UAR)」についての研究。UARの問題をMIL（Multiple Instance Learning）の一般化（GMIL）として扱い、ActivityNetなど大規模動画データから分布推定して表現を獲得。図は提案手法であるCross-Domain UAR (CD-UAR)である。ビデオから抽出したDeep特徴はGMILによりカーネル化される。Word2Vecとの投稿によりURを獲得し、ドメイン変換により新しい概念を獲得する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180323UAR.png" alt="180323UAR"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>従来法では見た/見てないの対応関係をデータセット中に含ませていたが、本論文での提案はUniversal Representation（ユニバーサル表現）を獲得して同タスクを解決する。</p></div></div><div class="item4"><div class="text"><h1>リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.08460.pdf">論文</a></li></ul></div></div><div class="slide_index">[#42]</div><div class="timestamp">2018.3.23 19:40:06</div></div></section><section><div class="paper-abstract"><div class="title">Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatial-Temporal Patterns</div><div class="info"><div class="authors">Jianming Lv, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>歩行者の時空間パターンを用いた、教師なし学習の人物再同定アルゴリズムであるTFusionを提案。既存の人物再同定アルゴリズムのほとんどは、小サイズのラベル付きデータセットを用いた教師付き学習手法である。そのため、大規模な実世界のカメラネットワークに適応することは困難である。また、そこで、ラベルなしデータセットも用いたクロスデータセット手法によって精度向上を図る。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330TFusion.jpg"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>まず、歩行者の空間的-時間的パターンを学習するために、ラベル付きデータセットを用いて学習した視覚的分類器を、ラベルなしデータセットに転送。次に、Bayesian fusion modelによって、学習された時空間パターンを視覚的特徴と組み合わせて、分類器を改善。最後に、ラベルのないデータを用いて分類器を段階的に最適化。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>人物再同定のための、教師なしクロスデータセット学習手法の中では最先端。</p><ul><li><a href="https://arxiv.org/pdf/1803.07293.pdf">論文</a></li></ul></div></div><div class="slide_index">[#43]</div></div></section><section><div class="paper-abstract"><div class="title">Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatio-temporal Patterns</div><div class="info"><div class="authors">Jianming, Lv and Weihang, Chen and Qing, Li and Can, Yang</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>ラベルなし、ドメインが異なる環境に対して人物再同定を行う手法を提案する。モデルであるTFusionは４ステップにより構築（１）教師あり学習により識別器を構築（２）ターゲットであるラベルなしデータにより時空間特徴パターン（Spatio-temporal Pattern）を学習（３）統合モデルFを学習（４）ラベルなしのターゲットデータにて徐々に識別器を学習する（１〜４は図に示されている）。Bayesian Fusionを提案して、時空間特徴パターンと人物のアピアランス特徴を統合してドメイン変換を行う。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180323CDReID.png" alt="180323CDReID"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>従来の人物再同定の設定では比較的小さいデータセットであり、完全に教師ありの環境を想定していたが、本論文ではラベルなし、ドメインが異なる環境に対して人物再同定を実行するため、非常に難しい問題となる。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><ul><li><a href="https://arxiv.org/pdf/1803.07293.pdf">論文</a></li><li><a href="https://github.com/ahangchen/TFusion">GitHub</a></li></ul></div></div><div class="slide_index">[#44]</div><div class="timestamp">2018.3.23 20:37:22</div></div></section><section><div class="paper-abstract"><div class="title">Unsupervised Textual Grounding: Linking Words to Image Concepts</div><div class="info"><div class="authors">Raymond A. Yeh, Minh N. Do, Alexander G. Schwing</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>単語を検出された画像の概念に関連付けるための、仮説検定を用いた教師なしTextual grounding手法の提案。ネットワークにはVGG-16を採用し、画像内のオブジェクト/単語の空間情報やクラス情報、およびクラス外の新しい概念を学習できる。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401UTG.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>Textual grounding、すなわち画像内のオブジェクトと単語をリンクさせる既存の技法は、教師付きのディープラーニングとして定式化されており、大規模なデータセットを用いてバウンディングボックスを推定する。しかし、データセットの構築には時間やコストがかかるので教師なしの手法を提案。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>ReferIt GameとFlickr30kを用いたベンチマークでそれぞれ7.98％と6.96％以上の精度。</p><ul><li><a href="https://arxiv.org/pdf/1803.11185.pdf">論文</a></li></ul></div></div><div class="slide_index">[#45]</div></div></section><section><div class="paper-abstract"><div class="title">Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments</div><div class="info"><div class="authors">Peter Anderson, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>自然言語のナビゲーションを入力として、実空間の中をロボットが動き目的地に到達できるかどうかを競うベンチマーク（Visually-grounded natural language navigation in real buildings）を提案。データセットは3Dのシミュレータによりキャプチャされ、22Kのナビゲーション、文章の平均単語数は29で構成される。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180305R2RNavi.png" alt="180305R2RNavi"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>(1) Matterport3Dデータセットを強化学習を行えるように拡張。(2) 同タスクが行えるようなベンチマークであるRoom-to-Room (R2R)を提案して言語と視覚情報から実空間にてナビができるようにした。(3) seq-to-seqをベースとしたニューラルネットによりベンチマークを構築。VQAをベースにしていて、ナビゲーション（VQAでいう質問文）と移動アクション（VQAでいう回答）という組み合わせで同問題を解決する。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>自然言語の問題はキャプションや質問回答の枠を超えて実空間、さらにいうとロボットタスクに導入されつつある。この研究はビジョン側からのアプローチだが、ロボット側のアプローチが現在どこまでできているか気になる。すでに屋内環境をある程度自由に移動するロボットが実現しているとこの実現可能性が高くなる。SLAMとの組み合わせももう実行できるレベルにある？</p><ul><li><a href="https://arxiv.org/pdf/1711.07280.pdf">論文</a></li><li><a href="https://bringmeaspoon.org/">Project</a></li><li><a href="https://github.com/peteanderson80/Matterport3DSimulator">GitHub</a></li><li><a href="https://niessner.github.io/Matterport/">Matterport3D dataset</a></li></ul></div></div><div class="slide_index">[#46]</div><div class="timestamp">2018.3.5 19:53:46</div></div></section><section><div class="paper-abstract"><div class="title">Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment</div><div class="info"><div class="authors">Li Ding, Chenliang Xu</div><div class="conference">CVPR 2018</div></div><div class="slide_editor"><a href="http://hirokatsukataoka.net/" target="blank">Hirokatsu Kataoka</a></div><div class="item1"><div class="text"><h1>概要</h1><p>時系列の行動検出/セグメンテーション（Action Segmentation）に関する問題をWeakly-Supervised（WS学習）に解いた。ここではTemporal Convolutional Feature Pyramid Network (TCFPN)とIterative Soft Boundary Assignment (ISBA)を繰り返すことで行動に関する条件学習ができてくるという仕組み。TCFPNではフレームの行動を予測し、ISBAではそれを検証、それらを繰り返して行動間の境界線を定めながらWS学習の教師としていく。さらに、WS学習を促進するためにより弱い境界として行動間の繋がりを定義することでWS学習の精度を向上させる。学習はビデオ単位の誤差を最適化することで境界についても徐々に定まる（ここがWS学習の所以）ように学習する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180329ISBATCFN.png" alt="180329ISBATCFN"></p></div></div><div class="item3"><div class="text"><h1>新規性・結果</h1><p>Breakfast dataset, Hollywood extended datasetにて弱教師付き学習とテストを行いState-of-the-artな精度を達成した。</p></div></div><div class="item4"><div class="text"><h1>コメント・リンク集</h1><p>弱い教師データを大量に集めると、そろそろ（ある程度の）教師ありデータによる精度を超えそう？もっと汎用的に学習できる枠組みが必要か。</p><ul><li><a href="https://arxiv.org/pdf/1803.10699v1.pdf">論文</a></li></ul></div></div><div class="slide_index">[#47]</div><div class="timestamp">2018.3.29 14:27:12</div></div></section><section><div class="paper-abstract"><div class="title">Who Let The Dogs Out? Modeling Dog Behavior From Visual Data</div><div class="info"><div class="authors">Kiana Ehsani, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>犬視点の大規模ビデオデータセットを作成し、このデータを使用した、犬の行動や行動計画のモデル化。次の3つの問題に焦点を当てる。(1)犬の行動予測。(2)入力された画像対から犬のような行動計画を見出す。(3)例えば、歩行可能な表面推定などのタスクについて、学習された表現を利用。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180401DogsOut.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>視覚情報からintelligent agent(知的エージェント)を直接的にモデリングするタスク。犬の視覚情報を使うことで、行動をモデル化する斬新な取り組み。得られたモデルをAIなどに応用する。特に、歩行可能な表面推定のタスクで良い結果となる。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>様々なエージェントやシナリオで使用でき、ラベルがないにもかかわらず有用な情報を学習することが可能。今後は、モデルやデーセットの拡張に挑む。</p><ul><li><a href="https://arxiv.org/pdf/1803.10827.pdf">論文</a></li></ul></div></div><div class="slide_index">[#48]</div></div></section><section><div class="paper-abstract"><div class="title">Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs</div><div class="info"><div class="authors">Xiaolong Wang, Yufei Ye, Abhinav Gupta, The Robotics Institute, Carnegie Mellon University</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>カテゴリの単語の埋め込みと他のカテゴリとの関係(視覚データが提供される)を使用するだけで、学習例がないカテゴリの分類器を学習するゼロショット認識モデルを提案。 knowledge graph (KG) を入力とし、Graph Convolutional Network(GCN)を基に、セマンティック埋め込みとカテゴリの関係の両方を使用して分類器を予測する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330KG.jpg"></p></div></div><div class="item3"><div class="text"><h1>手法</h1><p>学習済のKGが与えられると、各ノードに対する意味的埋め込みとして入力を得る。一連のグラフ畳み込みの後、各カテゴリの視覚的分類器を予測する。トレーニング中に、カテゴリの視覚的分類器が与えられ、GCNパラメータを学習。テスト時に、これらのフィルタを使用して、見えないカテゴリの視覚的分類器を予測する。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>KGのノイズに対してロバストであり、最先端の精度。</p><ul><li><a href="https://arxiv.org/pdf/1803.08035.pdf">論文</a></li></ul></div></div><div class="slide_index">[#49]</div></div></section><section><div class="paper-abstract"><div class="title">Zoom and Learn: Generalizing Deep Stereo Matching to Novel Domains</div><div class="info"><div class="authors">Jiahao Pang, et al.</div><div class="conference">CVPR 2018</div></div><div class="slide_editor">Munetaka Minoguchi</div><div class="item1"><div class="text"><h1>概要</h1><p>学習済みデータと新しいドメイン(ground-truthなし)の両方を用いて、ディープステレオマッチングを行うZoom and Lean(ZOLE)の提案。これにより，他のドメインに一般化できるプレトレインモデルを作成することができる。一般化に際する不具合を抑制しながらアップサンプリングを行う、反復最適化問題を定式化する。</p></div></div><div class="item2"><div class="text"><p><img src="slides/figs/180330ZOLE.jpg"></p></div></div><div class="item3"><div class="text"><h1>新規性</h1><p>ground-truthデータが不足しているため、CNNを用いたステレオマッチングでは学習済みステレオモデルを新規ドメインに一般化することが困難とされていた。CNN学習時のイテレーションごとに最適化していくイメージ。</p></div></div><div class="item4"><div class="text"><h1>結果・リンク集</h1><p>スマートフォンで収集したデータを従来の手法に入力すると、物体のエッジがぼやけてしまうが、提案手法のZOLEではこれらを改善できる。</p><ul><li><a href="https://arxiv.org/pdf/1803.06641.pdf">論文</a></li></ul></div></div><div class="slide_index">[#50]</div></div></section></div></div><script src="lib/js/head.min.js"></script><script src="js/reveal.js"></script><script>Reveal.initialize({
  history: true,
  center: false,
  width: '100%',
  height: '100%',
  transition: 'none',
  dependencies: [
    { src: 'plugin/markdown/marked.js' },
    { src: 'plugin/markdown/markdown.js' },
    { src: 'plugin/notes/notes.js', async: true },
    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
  ]
});</script></body></html>